# ğŸªœ **Breaking the Project into Smaller Parts (Short & Clear)**

### **Phase 1: OCEAN Personality Simulation**

1. **Build Synthetic Agents** â†’ Create 1,000 AI agents (`synthetic_agents.json`) with demographic and personality traits.  
2. **Collect Human Responses** â†’ Two real human datasets (`phase1`, `phase2`) used for validation.  
3. **Run OCEAN Questions** â†’ Agents answer Big Five (OCEAN) personality questions using:
   - **Cloud-based LLMs** via OpenAI API.
   - **Local LLM** (e.g., Ollama / Llama-3).  
4. **Compute Metrics** â†’ Compare Human vs Agent responses using:
   - **Correlation**
   - **KL Divergence**
   - **JS Divergence**
   - **Normalized Accuracy**
5. **Generate Visualizations** â†’ Produce metrics and comparison charts under `results/ocean_results/`.

---

### **Phase 2: Media Survey Simulation**

6. **Collect Media Poll Data** â†’ Real-world public poll questions (2020â€“2025).  
7. **Run Agent Responses** â†’ Agents answer identical media poll questions using both cloud and local models.  
8. **Compare with Humans** â†’ Evaluate trends and response alignment.  
9. **Visualize Media Results** â†’ Generate comparative charts for percentage distributions and time trends.

---

### **Phase 3: Deliverables**

10. **Datasets** â†’ Synthetic agent responses + Human datasets (`phase1`, `phase2`).  
11. **Analysis Outputs** â†’ Charts for correlation, KL/JS divergence, and normalized accuracy.  
12. **Final Report** â†’ Methodology, results, and summary of human-agent alignment.  
13. **Presentation** â†’ Slide deck summarizing pipeline, results, and charts.

---

## ğŸ§© **Key Aspects**

**Agent Design**  
- 1,000 OCEAN-based synthetic agents.  
- Each agent answers using either Cloud API or Local LLM pipeline.  

**Datasets**
- `OCEAN Dataset` â†’ Big Five Personality Questions  
- `Media Dataset` â†’ Public opinion poll questions (2020 vs 2025)

**Pipeline**
1. Load Agents + Questions.  
2. Build prompts (Cloud or Local).  
3. Execute simulations.  
4. Store all responses in CSV/JSON under `results/`.  
5. Compute metrics & plot graphs.

**Analysis**
- Compare Human vs Agent distributions.
- Compute accuracy, divergence, and consistency scores.
- Visualize metrics across runs.

---

## ğŸ“‚ **Project Structure**

TEAMROSS/capstone3/
â”œâ”€â”€ agents/
â”‚ â””â”€â”€ synthetic_agents.json
â”‚
â”œâ”€â”€ human/
â”‚ â”œâ”€â”€ human_responses_phase1.csv
â”‚ â”œâ”€â”€ human_responses_phase2.csv
â”‚ â”œâ”€â”€ human_vs_human_metrics_250.csv / .png
â”‚ â”œâ”€â”€ human_vs_human_metrics_1000.csv / .png
â”‚
â”œâ”€â”€ pipeline/
â”‚ â”œâ”€â”€ CLOUD_API/
â”‚ â”‚ â”œâ”€â”€ Runagent_cloud.py
â”‚ â”‚ â”œâ”€â”€ compare_HVA_1000.py
â”‚ â”‚ â”œâ”€â”€ compare_ocean_HVA500.py
â”‚ â”‚ â”œâ”€â”€ normalize_acc_1000.py
â”‚ â”‚ â””â”€â”€ cloud_vs_cloud250.py
â”‚ â”œâ”€â”€ LOCAL_LLM/
â”‚ â”‚ â”œâ”€â”€ compare_localvshuman.py
â”‚ â”‚ â””â”€â”€ run_agents.py
â”‚ â”œâ”€â”€ HUMAN_VS_HUMAN/
â”‚ â”‚ â”œâ”€â”€ compare_HVH_250.py
â”‚ â”‚ â””â”€â”€ compre_ocean_HVH_1000.py
â”‚ â”œâ”€â”€ 5_STUDIES/ # Future Extension
â”‚ â”‚ â”œâ”€â”€ ames_and_frisky.py
â”‚ â”‚ â”œâ”€â”€ cooney_et_al.py
â”‚ â”‚ â”œâ”€â”€ halevy_halali.py
â”‚ â”‚ â”œâ”€â”€ rai_et_al_final.py
â”‚ â”‚ â””â”€â”€ schilke_reimann_cook.py
â”‚
â”œâ”€â”€ questions/
â”‚ â”œâ”€â”€ OCEAN.json
â”‚ â””â”€â”€ media.json
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ ocean_results/ # OCEAN metrics
â”‚ â”œâ”€â”€ media/ # Media metrics
â”‚ â””â”€â”€ study_results/ # (future studies)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ **Setup Instructions**

1. **Clone this repo** (or copy project folder).  
2. **Create a virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate       # macOS/Linux
   venv\Scripts\activate          # Windows


## Install dependencies:

bash
Copy code
pip install -r requirements.txt
Add API Key for cloud runs:
Create a .env file in the project root and add:

ini
Copy code
OPENAI_API_KEY=your_openai_api_key_here

## Running the Pipeline

1ï¸âƒ£ Generate Agent Responses (Cloud)
python pipeline/CLOUD_API/Runagent_cloud.py

2ï¸âƒ£ Generate Agent Responses (Local LLM)
python pipeline/LOCAL_LLM/run_agents.py

3ï¸âƒ£ Compare Human vs Agent (Cloud)
python pipeline/CLOUD_API/compare_HVA_1000.py

4ï¸âƒ£ Compare Human vs Agent (Local)
python pipeline/LOCAL_LLM/compare_localvshuman.py

5ï¸âƒ£ Validate Human Internal Consistency
python pipeline/HUMAN_VS_HUMAN/compare_HVH_250.py



# Example Outputs
## OCEAN Results (Human vs Agent)
File	Description
normalized_accuracy_cloud1000.png	Normalized accuracy of agentâ€“human responses
Correlation_UHCL_Hawks_Final.png	Correlation plot for OCEAN metrics
KL_Divergence_UHCL_Hawks_Final.png	KL Divergence (Human vs Agent distributions)
JS_Divergence_UHCL_Hawks_Final.png	JS Divergence indicating behavioral overlap

# Example Charts


Figure 1 â€“ Normalized accuracy (Human vs Agent).


Figure 2 â€“ JS divergence showing distribution similarity.

Media Results (Poll Comparisons)
File	Description
media_metrics_bar.png	Overall media poll results (agents vs humans)
media_metrics_average.png	Average comparison for multiple questions
media_q1_trend_chart.png	2020 vs 2025 agent response trends for question 1

# Example Charts


Figure 3 â€“ Human vs Agent response percentages.


Figure 4 â€“ Temporal agent response trends.

# Metric Interpretation
Metric	Ideal Range	Interpretation
	
KL < 0.05	Minimal information loss	
JS < 0.02	>98% behavioral similarity	

