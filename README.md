# ğŸ§  Generative Reflector Units (RUs)  
### *Simulating 1,000 Human-like Respondents through Local and Cloud LLM Execution*

---

## âš™ï¸ Part 1 â€“ Project Setup

### ğŸ§© 1. System Requirements
| Tool | Purpose |
|------|----------|
| **Python 3.10 or later** | Runs all RU scripts |
| **Anaconda / Miniconda (or venv)** | Creates a clean virtual environment |
| **Git** | Clone this repository |
| **OpenAI API Key (Cloud mode)** | Get from https://platform.openai.com |
| **Ollama (optional)** | Needed only for Local LLM mode (e.g., Llama 3) |

---

### ğŸ§± 2. Clone the Repository
```bash
git clone https://github.com/<yourusername>/capstone3.git
cd capstone3
```

---

### ğŸ§® 3. Create and Activate a Virtual Environment
**Conda (recommended):**
```bash
conda create -n capstone3 python=3.12 -y
conda activate capstone3
```
**or venv:**
```bash
python -m venv venv
source venv/bin/activate        # macOS / Linux
venv\Scripts\activate           # Windows
```

---

### ğŸ“¦ 4. Install Dependencies
```bash
pip install -r requirements.txt
```

If missing, create:
```bash
# requirements.txt
openai
python-dotenv
pandas
numpy
matplotlib
scipy
tqdm
```

---

### ğŸ”‘ 5. Add OpenAI API Key (for Cloud Mode)
Create `.env` in project root:
```
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
```

---

### ğŸ§  6. Install Ollama (for Local Mode)
```bash
# macOS
brew install ollama
# Windows
winget install Ollama.Ollama
```
Then download the model:
```bash
ollama pull llama3
```

> Ollama runs system-wide; the Python environment simply connects to it.

---

### ğŸ§¾ 7. Verify Setup
```bash
python -c "import openai, pandas, numpy, matplotlib, scipy; print('âœ… All dependencies installed successfully!')"
```

---

## ğŸš€ Part 2A â€“ Running Reflector Units in Local Mode (Ollama + Llama 3)

### ğŸ§© 1. Start Ollama Service
```bash
ollama serve
```
Keep this terminal open while running RUs.

---

### ğŸ§° 2. Check Model
```bash
ollama list
# If missing:
ollama pull llama3
```

---

### ğŸ§® 3. Activate Environment
```bash
conda activate capstone3
# or
source venv/bin/activate
```

---

### ğŸ§¾ 4. Run Reflector Units
```bash
python RUS/run_RUS_LLM.py
```

The script will load RU profiles (`RUS/synthetic_RUs.json`), read questions from `questions/`, send prompts to Llama 3 through Ollama, and save results to `results/`.

---

### ğŸ“ 5. Outputs
```
results/
 â””â”€â”€ media/
      â”œâ”€â”€ media_RUs.csv
      â”œâ”€â”€ media_log.json
      â”œâ”€â”€ response_snapshots/
      â””â”€â”€ batch_metrics.csv
```

---

### âš ï¸ 6. Troubleshooting
| Issue | Cause | Fix |
|-------|--------|-----|
| `ConnectionRefusedError` | Ollama not running | `ollama serve` |
| `Model llama3 not found` | Model not downloaded | `ollama pull llama3` |
| `ModuleNotFoundError` | Missing packages | `pip install -r requirements.txt` |
| Slow responses | Heavy CPU/RAM load | Reduce batch size in script |

---

### ğŸ§© 7. Notes
- Works completely offline once Llama 3 is downloaded.  
- Adjust batch size or temperature inside `run_RUS_LLM.py`.  
- Logs stored in `results/local_logs/`.

---

## â˜ï¸ Part 2B â€“ Running Reflector Units in Cloud Mode (OpenAI API)

### ğŸ”‘ 1. Confirm API Key
`.env` must contain:
```
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
```

---

### ğŸ§© 2. Activate Environment
```bash
conda activate capstone3
# or
source venv/bin/activate
```

---

### ğŸš€ 3. Run Cloud Mode
```bash
python RUS/run_RUS_cloud.py
```

The script loads Reflector Unit profiles and questions, calls **GPT-4o-mini**, and stores responses and metrics under `results/`.

---

### ğŸ“ 4. Outputs
```
results/
 â””â”€â”€ media/
      â”œâ”€â”€ media_RUs_cloud.csv
      â”œâ”€â”€ cloud_run_log.json
      â”œâ”€â”€ batch_metrics_cloud.csv
      â””â”€â”€ comparison_graphs/
```

---

### âš ï¸ 5. Common Errors
| Issue | Cause | Fix |
|-------|--------|-----|
| `AuthenticationError` | Invalid API key | Re-check `.env` |
| `RateLimitError` | Too many requests | Lower batch size / add delays |
| `FileNotFoundError` | Wrong path | Verify file paths |
| `Timeout` | Slow internet / large batch | Re-run smaller batches |

---

### ğŸ“Š 6. Usage Tips
- Monitor token usage on OpenAI dashboard.  
- Adjust `temperature`, `max_tokens`, `batch_size` inside `run_RUS_cloud.py`.  
- Results auto-timestamp in `results/`.

---

## ğŸ“Š Part 3 â€“ Comparing Human vs Reflector Unit Results

### ğŸ§© 1. Required Files
| File | Description |
|------|--------------|
| `results/media/media_RUs.csv` or `media_RUs_cloud.csv` | RU responses |
| `results/media/media_human_resp.json` | Human survey data |

---

### ğŸš€ 2. Run Comparison
```bash
python pipeline/compare_human_vs_RU.py
```

Computes KL-Divergence, JS-Divergence, and t-tests, then plots graphs.

---

### ğŸ“ 3. Outputs
```
results/media/
 â”œâ”€â”€ KL_JS_metrics.csv
 â”œâ”€â”€ human_vs_RU_summary.csv
 â””â”€â”€ comparison_graphs/
      â”œâ”€â”€ kl_divergence.png
      â”œâ”€â”€ js_divergence.png
      â””â”€â”€ distribution_overlap.png
```

---

### ğŸ“ˆ 4. Metric Interpretation
| Metric | Meaning |
|---------|----------|
| **KL â†“** | Smaller = closer to human |
| **JS â†“** | Symmetric distance (0 â‰ˆ perfect) |
| **t-Test p â†‘** | > 0.05 â†’ no significant difference |
| **Consistency Î± â†‘** | Higher = more stable RUs |

Example: `KL 0.028  JS 0.014  Î± 0.91`

---

## ğŸ§© Part 4 â€“ Visualization and Result Interpretation

### ğŸ–¼ï¸ 1. Graph Location
```
results/media/comparison_graphs/
```

Files: `kl_divergence.png`, `js_divergence.png`, `distribution_overlap.png`, `batch_consistency.png`

---

### ğŸ§® 2. Regenerate Plots
```bash
python pipeline/metrics_visualizer.py
```

---

### ğŸ“Š 3. How to Read Charts
| Plot | Shows | Read As |
|------|--------|---------|
| KL Bar Chart | Info loss RUâ†’Human | Lower = better |
| JS Heatmap | Similarity across topics | Cooler colors = closer |
| Distribution Overlap | Probabilities per question | More overlap = similar |
| Consistency Histogram | Stability per batch | Peaks near 1.0 = good |

---

### ğŸ§  4. Alignment Quality
| Range | Quality | Meaning |
|--------|----------|----------|
| 0â€“0.02 | Excellent | Almost human-like |
| 0.02â€“0.05 | Good | Minor variation |
| 0.05â€“0.10 | Moderate | Some topic shift |
| > 0.10 | Low | Needs tuning |

---

### ğŸ§© 5. Tips for Reports
- Include Local vs Cloud comparisons.  
- Mention internal consistency Î± values.  
- Label plots clearly as â€œRUs vs Humansâ€.

---

## ğŸ—‚ï¸ Part 5 â€“ Project Folder Structure and Execution Flow

### ğŸ“ 1. Folder Layout
```
capstone3/
â”œâ”€â”€ RUS/
â”‚   â”œâ”€â”€ run_RUS_LLM.py
â”‚   â”œâ”€â”€ run_RUS_cloud.py
â”‚   â””â”€â”€ synthetic_RUs.json
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”œâ”€â”€ reflection_manager.py
â”‚   â”œâ”€â”€ plan_manager.py
â”‚   â”œâ”€â”€ compare_human_vs_RU.py
â”‚   â”œâ”€â”€ internal_consistency.py
â”‚   â””â”€â”€ metrics_visualizer.py
â”‚
â”œâ”€â”€ questions/
â”‚   â”œâ”€â”€ media_questions.json
â”‚   â”œâ”€â”€ psychometrics.json
â”‚   â”œâ”€â”€ classic_studies.json
â”‚   â””â”€â”€ uhcl_survey.json
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ media/
â”‚   â””â”€â”€ local_logs/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ”„ 2. Execution Flow
```
Reflector Units (RUs)
     â”‚
     â–¼
MemoryManager â†’ ReflectionManager â†’ PlanManager
     â”‚
     â–¼
Response Generation (Llama 3 or GPT-4o-mini)
     â”‚
     â–¼
Results Storage (CSV/JSON)
     â”‚
     â–¼
Human vs RU Comparison (KL, JS)
     â”‚
     â–¼
Visualization & Metrics Plots
```

---

### ğŸ§  3. Step Summary
| Step | Module | Input | Output |
|------|---------|--------|---------|
| 1 | `run_RUS_LLM.py` / `run_RUS_cloud.py` | RU profiles + questions | RU responses (CSV/JSON) |
| 2 | `compare_human_vs_RU.py` | Human + RU data | KL/JS metrics |
| 3 | `internal_consistency.py` | RU responses | Î± (reliability) |
| 4 | `metrics_visualizer.py` | Metric CSVs | PNG graphs |

---

âœ… **Setup complete and ready for execution.**  
Run either Local or Cloud mode, compare results, and review the plots in `results/media/comparison_graphs/`.


### ğŸ§¾ License
MIT License â€“ Free to use and modify with attribution.

